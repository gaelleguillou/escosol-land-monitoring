{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34ac563",
   "metadata": {},
   "source": [
    "# Analyse des stratégies\n",
    "\n",
    "## Site-wide research\n",
    "\n",
    "Il existe une recherche site-wide qui nous permettrait de tout faire en une query.\n",
    "https://www.mrae.developpement-durable.gouv.fr/?typedoc=pdf&recherche=photovolta%C3%AFque&page=recherche&perimetre=all\n",
    "Le risque est que ça nous permet pas de filtrer les \"avis sur projets\" précis.\n",
    "\n",
    "## Crawling + scraping\n",
    "\n",
    "A partir de la base d'url qui nous a été données, on peut crawl les projets par année et mois (ce qui nous donne déjà de la donnée formatée) en récupérant seulement les PDFs qui indiquent \"photovoltaïque\" ou \"photovoltaique\" dans la description.\n",
    "\n",
    "Les pages avis rendus sur projet ont toujours le même format : un lien par année (dont un pour archives).\n",
    "\n",
    "Etapes : \n",
    "\n",
    "1 - Récupérer les URLs uniques par année\n",
    "\n",
    "2 - Scraper les pages avis / année en filtrant sur photovoltaïque dans la description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816107eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access pages and extract links to avis projet x année\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "region_links = pd.read_csv(\"data/region_links_ae.csv\")\n",
    "\n",
    "def get_mrae_links(base_url=None, region=None):\n",
    "\n",
    "    timeout_config = httpx.Timeout(60.0, connect=10.0)\n",
    "        \n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "    all_links = []\n",
    "    current_url = base_url\n",
    "\n",
    "    with httpx.Client(headers=headers, timeout=timeout_config, follow_redirects=True) as client:\n",
    "        while current_url:\n",
    "            response = client.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            links = soup.select(\"h2.fr-card__title a\")\n",
    "            for a in links:\n",
    "                full_url = urljoin(base_url, a[\"href\"])\n",
    "                year = re.search(r\"(\\d{4})\", a[\"title\"]).group(1)\n",
    "                all_links.append([base_url, region, year, full_url])\n",
    "            \n",
    "            next_page = soup.select_one(\"a.fr-pagination__link--next\")\n",
    "            if next_page and next_page.get(\"href\"):\n",
    "                current_url = urljoin(base_url, next_page[\"href\"])\n",
    "            else:\n",
    "                current_url = None\n",
    "                \n",
    "    return all_links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = []\n",
    "    for _, row in region_links.iterrows():\n",
    "        data = get_mrae_links(row[\"site\"], row[\"region\"])\n",
    "        results.extend(data)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.columns = [\"base_url\", \"region\", \"year\", \"year_url\"]\n",
    "    df_results.to_csv(\"data/ae_year_links.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "escosol-monitoring (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
